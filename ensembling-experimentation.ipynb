{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/universal-sentence-encoder/use/use/tfhub_module.pb\n",
      "/kaggle/input/universal-sentence-encoder/use/use/saved_model.pb\n",
      "/kaggle/input/universal-sentence-encoder/use/use/variables/variables.data-00000-of-00001\n",
      "/kaggle/input/universal-sentence-encoder/use/use/variables/variables.index\n",
      "/kaggle/input/ift3395-ift6390-reddit-comments/sample_submission.csv\n",
      "/kaggle/input/ift3395-ift6390-reddit-comments/data_test.pkl\n",
      "/kaggle/input/ift3395-ift6390-reddit-comments/data_train.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import csv\n",
    "import gensim\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.backend import one_hot, clear_session\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost as xgb\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_predictions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() # Needed for TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['train', 'test']\n",
    "input_path = '/kaggle/input/ift3395-ift6390-reddit-comments/'\n",
    "data_train, data_test = [np.load(os.path.join(input_path, f'data_{dataset}.pkl'), allow_pickle=True) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(data):\n",
    "    if len(data) == 2:\n",
    "        comment, label = data\n",
    "        result = pd.DataFrame({'comment': comment, 'label': label})\n",
    "    else:\n",
    "        result = pd.DataFrame({'comment': data})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_FRACTION = 0.1\n",
    "train_val_df, test_df = (to_dataframe(data) \n",
    "                     for data in [data_train, data_test])\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=VAL_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_labels(labels):\n",
    "    return np.array(labels)[:, np.newaxis]\n",
    "\n",
    "def to_dense(*args):\n",
    "    return [item.todense() if hasattr(item, 'todense') else item for item in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USEModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classes, use=None):\n",
    "        if not use:\n",
    "            use = hub.Module('../input/universal-sentence-encoder/use/use')\n",
    "        self.classes = classes\n",
    "        self.class_count = len(classes)\n",
    "        self.use = use\n",
    "        self.one_hot_encoder = OneHotEncoder()\n",
    "        self.one_hot_encoder.fit(vectorize_labels(classes))\n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        X, y = data\n",
    "        X_emb = sess.run(self.use(X))\n",
    "        y_onehot = self.one_hot_encoder.transform(y[:, np.newaxis]).todense()\n",
    "        return X_emb, y_onehot\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.train_data = self.preprocess((x, y))\n",
    "        self.idxmap = np.squeeze(\n",
    "            np.array([np.where(self.one_hot_encoder.categories_[0] == class_label)\n",
    "                      for class_label in self.classes]))        \n",
    "        self.build_model(self.train_data[0].shape[1])\n",
    "        self.train()\n",
    "        \n",
    "    def build_model(self, input_shape):\n",
    "        inputs = layer = L.Input(shape=input_shape, dtype=float)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dropout(.2)(layer)\n",
    "        layer = L.Dense(self.class_count, activation='softmax')(inputs)\n",
    "        self.model = Model(inputs=inputs, outputs=layer)\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['acc'])        \n",
    "\n",
    "    def train(self, epochs=40):\n",
    "        print(\"Training\")\n",
    "        X_train, y_train = self.train_data\n",
    "        self.model.fit(X_train, y_train, epochs=epochs)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_emb = sess.run(self.use(X))        \n",
    "        probs = self.model.predict(X_emb)\n",
    "        return probs[:, self.idxmap]\n",
    "\n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes[idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfNaiveBayesModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classes, alpha=.42):\n",
    "        self.classes = np.array(classes)\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.naive = MultinomialNB(alpha=alpha)        \n",
    "    def fit(self, x, y):\n",
    "        vectors = self.tfidf.fit_transform(x)\n",
    "        self.naive.fit(vectors, y)\n",
    "        self.idxmap = np.squeeze(\n",
    "            np.array([np.where(self.naive.classes_ == class_label)\n",
    "                      for class_label in self.classes]))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        vectors = self.tfidf.transform(x)\n",
    "        probabilities = self.naive.predict_proba(vectors)\n",
    "        return probabilities[:, self.idxmap]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVotingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimators, classes):\n",
    "        self.estimators = estimators\n",
    "        self.classes = classes\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        for estimator in self.estimators:\n",
    "            estimator.fit(x, y)\n",
    "            \n",
    "    def predict_proba(self, x):\n",
    "        probs = np.array(\n",
    "            [estimator.predict_proba(x) for estimator in self.estimators])\n",
    "        probs = np.sum(probs, axis=0) / len(self.estimators)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes[idx]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "use = hub.Module('../input/universal-sentence-encoder/use/use')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_val_df.label.unique()\n",
    "use_model = USEModel(classes, use)\n",
    "naive_model = TfidfNaiveBayesModel(classes)\n",
    "classifiers = [use_model, naive_model]\n",
    "voting = SimpleVotingClassifier(estimators=classifiers, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not submit_predictions:\n",
    "    voting.fit(train_df.comment, train_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = voting.estimators[0]\n",
    "self.idxmap = np.squeeze([np.where(self.one_hot_encoder.categories_[0] == class_label)\n",
    "                      for class_label in self.classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not submit_predictions:\n",
    "    predictions = voting.predict(val_df.comment)\n",
    "    accuracy = np.sum(predictions == val_df.label) / len(val_df)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not submit_predictions:\n",
    "    predictions = voting.estimators[0].predict(val_df.comment)\n",
    "    accuracy = np.sum(predictions == val_df.label) / len(val_df)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train on 70000 samples\n",
      "Epoch 1/40\n",
      "70000/70000 [==============================] - 6s 79us/sample - loss: 1.7743 - acc: 0.5767\n",
      "Epoch 2/40\n",
      "70000/70000 [==============================] - 5s 65us/sample - loss: 1.3411 - acc: 0.6044\n",
      "Epoch 3/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.3024 - acc: 0.6086\n",
      "Epoch 4/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2892 - acc: 0.6111\n",
      "Epoch 5/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2820 - acc: 0.6128\n",
      "Epoch 6/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2772 - acc: 0.6134\n",
      "Epoch 7/40\n",
      "70000/70000 [==============================] - 4s 62us/sample - loss: 1.2734 - acc: 0.6154\n",
      "Epoch 8/40\n",
      "70000/70000 [==============================] - 4s 64us/sample - loss: 1.2706 - acc: 0.6155\n",
      "Epoch 9/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2681 - acc: 0.6169\n",
      "Epoch 10/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2662 - acc: 0.6172\n",
      "Epoch 11/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2642 - acc: 0.6175\n",
      "Epoch 12/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2627 - acc: 0.6179\n",
      "Epoch 13/40\n",
      "70000/70000 [==============================] - 4s 62us/sample - loss: 1.2610 - acc: 0.6180\n",
      "Epoch 14/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2597 - acc: 0.6194\n",
      "Epoch 15/40\n",
      "70000/70000 [==============================] - 5s 65us/sample - loss: 1.2583 - acc: 0.6193\n",
      "Epoch 16/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2571 - acc: 0.6196\n",
      "Epoch 17/40\n",
      "70000/70000 [==============================] - 4s 62us/sample - loss: 1.2557 - acc: 0.6202\n",
      "Epoch 18/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2547 - acc: 0.6203\n",
      "Epoch 19/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2536 - acc: 0.6207\n",
      "Epoch 20/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2526 - acc: 0.6210\n",
      "Epoch 21/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2516 - acc: 0.6209\n",
      "Epoch 22/40\n",
      "70000/70000 [==============================] - 4s 64us/sample - loss: 1.2506 - acc: 0.6210\n",
      "Epoch 23/40\n",
      "70000/70000 [==============================] - 4s 62us/sample - loss: 1.2497 - acc: 0.6215\n",
      "Epoch 24/40\n",
      "70000/70000 [==============================] - 4s 62us/sample - loss: 1.2487 - acc: 0.6219\n",
      "Epoch 25/40\n",
      "70000/70000 [==============================] - 4s 62us/sample - loss: 1.2479 - acc: 0.6230\n",
      "Epoch 26/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2470 - acc: 0.6219\n",
      "Epoch 27/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2463 - acc: 0.6231\n",
      "Epoch 28/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2455 - acc: 0.6228\n",
      "Epoch 29/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2447 - acc: 0.6236\n",
      "Epoch 30/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2439 - acc: 0.6232\n",
      "Epoch 31/40\n",
      "70000/70000 [==============================] - 4s 62us/sample - loss: 1.2433 - acc: 0.6239\n",
      "Epoch 32/40\n",
      "70000/70000 [==============================] - 4s 62us/sample - loss: 1.2426 - acc: 0.6239\n",
      "Epoch 33/40\n",
      "70000/70000 [==============================] - 4s 61us/sample - loss: 1.2419 - acc: 0.6240\n",
      "Epoch 34/40\n",
      "70000/70000 [==============================] - 4s 61us/sample - loss: 1.2412 - acc: 0.6245\n",
      "Epoch 35/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2405 - acc: 0.6248\n",
      "Epoch 36/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2400 - acc: 0.6244\n",
      "Epoch 37/40\n",
      "70000/70000 [==============================] - 4s 64us/sample - loss: 1.2393 - acc: 0.6246\n",
      "Epoch 38/40\n",
      "70000/70000 [==============================] - 4s 63us/sample - loss: 1.2387 - acc: 0.6245\n",
      "Epoch 39/40\n",
      "70000/70000 [==============================] - 4s 64us/sample - loss: 1.2381 - acc: 0.6250\n",
      "Epoch 40/40\n",
      "70000/70000 [==============================] - 4s 62us/sample - loss: 1.2376 - acc: 0.6253\n",
      "Sanity check accuracy: 0.6812857142857143 (not a true validation accuracy)\n"
     ]
    }
   ],
   "source": [
    "if submit_predictions:\n",
    "    voting.fit(train_val_df.comment, train_val_df.label)\n",
    "    sanity_check_predictions = voting.predict(val_df.comment)\n",
    "    accuracy = np.sum(sanity_check_predictions == val_df.label) / len(val_df)\n",
    "    print(f\"Sanity check accuracy: {accuracy} (not a true validation accuracy)\")\n",
    "    \n",
    "    predictions = voting.predict(data_test)\n",
    "    with open(\"predictions.csv\", 'w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Id\", \"Category\"])\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            wr.writerow((i,prediction))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
