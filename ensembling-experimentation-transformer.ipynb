{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\r\n",
      "\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from transformers) (4.36.1)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers) (0.1.83)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from transformers) (2019.8.19)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.10.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers) (1.16.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.22.0)\r\n",
      "Collecting sacremoses\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\r\n",
      "\u001b[K     |████████████████████████████████| 860kB 30.3MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.2.1)\r\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.2 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (1.13.2)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.8)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.24.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2019.9.11)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.12.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.13.2)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.2->boto3->transformers) (2.8.0)\r\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.2->boto3->transformers) (0.15.2)\r\n",
      "Building wheels for collected packages: sacremoses\r\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=f3e49ef6b52aff7494e084707f1aaf7048490745a32cfdabccc6551138046248\r\n",
      "  Stored in directory: /tmp/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\r\n",
      "Successfully built sacremoses\r\n",
      "Installing collected packages: sacremoses, transformers\r\n",
      "Successfully installed sacremoses-0.0.35 transformers-2.1.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/universal-sentence-encoder/use/use/tfhub_module.pb\n",
      "/kaggle/input/universal-sentence-encoder/use/use/saved_model.pb\n",
      "/kaggle/input/universal-sentence-encoder/use/use/variables/variables.index\n",
      "/kaggle/input/universal-sentence-encoder/use/use/variables/variables.data-00000-of-00001\n",
      "/kaggle/input/ift3395-ift6390-reddit-comments/data_test.pkl\n",
      "/kaggle/input/ift3395-ift6390-reddit-comments/sample_submission.csv\n",
      "/kaggle/input/ift3395-ift6390-reddit-comments/data_train.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import csv\n",
    "import gensim\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.backend import one_hot, clear_session\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizer, TFXLNetModel, XLNetTokenizer\n",
    "import xgboost as xgb\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_predictions = False\n",
    "train_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf2\n",
    "import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior() # Needed for TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['train', 'test']\n",
    "input_path = '/kaggle/input/ift3395-ift6390-reddit-comments/'\n",
    "data_train, data_test = [np.load(os.path.join(input_path, f'data_{dataset}.pkl'), allow_pickle=True) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(data):\n",
    "    if len(data) == 2:\n",
    "        comment, label = data\n",
    "        result = pd.DataFrame({'comment': comment, 'label': label})\n",
    "    else:\n",
    "        result = pd.DataFrame({'comment': data})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_FRACTION = 0.1\n",
    "train_val_df, test_df = (to_dataframe(data) \n",
    "                     for data in [data_train, data_test])\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=VAL_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_labels(labels):\n",
    "    return np.array(labels)[:, np.newaxis]\n",
    "\n",
    "def to_dense(*args):\n",
    "    return [item.todense() if hasattr(item, 'todense') else item for item in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USEModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classes, use=None):\n",
    "        if not use:\n",
    "            use = hub.Module('../input/universal-sentence-encoder/use/use')\n",
    "        self.classes = classes\n",
    "        self.class_count = len(classes)\n",
    "        self.use = use\n",
    "        self.one_hot_encoder = OneHotEncoder()\n",
    "        self.one_hot_encoder.fit(vectorize_labels(classes))\n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        X, y = data\n",
    "        X_emb = sess.run(self.use(X))\n",
    "        y_onehot = self.one_hot_encoder.transform(y[:, np.newaxis]).todense()\n",
    "        return X_emb, y_onehot\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.train_data = self.preprocess((x, y))\n",
    "        self.idxmap = np.squeeze(\n",
    "            np.array([np.where(self.one_hot_encoder.categories_[0] == class_label)\n",
    "                      for class_label in self.classes]))        \n",
    "        self.build_model(self.train_data[0].shape[1])\n",
    "        self.train()\n",
    "        \n",
    "    def build_model(self, input_shape):\n",
    "        inputs = layer = L.Input(shape=input_shape, dtype=float)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dropout(.2)(layer)\n",
    "        layer = L.Dense(self.class_count, activation='softmax')(inputs)\n",
    "        self.model = Model(inputs=inputs, outputs=layer)\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['acc'])        \n",
    "\n",
    "    def train(self, epochs=40):\n",
    "        print(\"Training\")\n",
    "        X_train, y_train = self.train_data\n",
    "        self.model.fit(X_train, y_train, epochs=epochs)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        X_emb = sess.run(self.use(x))        \n",
    "        probs = self.model.predict(X_emb)\n",
    "        return probs[:, self.idxmap]\n",
    "\n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes[idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel:\n",
    "    def __init__(self, classes, base=None, max_sequence_length=512, emb_batch_size=100):\n",
    "        self.classes = classes\n",
    "        self.class_count = len(classes)\n",
    "        if not base:\n",
    "            base = self.get_default_base()\n",
    "        self.transformer_eff = self.build_transformer_eff()\n",
    "        self.transformer, self.tokenizer = base            \n",
    "        self.one_hot_encoder = OneHotEncoder()\n",
    "        self.one_hot_encoder.fit(vectorize_labels(classes))\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.emb_batch_size = emb_batch_size\n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def build_transformer_eff(self):\n",
    "        @tf2.function\n",
    "        def eff_model(x):\n",
    "            outputs = self.transformer(x)\n",
    "            return tf.reduce_mean(outputs[0], axis=1)        \n",
    "        return eff_model\n",
    "\n",
    "    def get_default_base(self):\n",
    "        model_desc = (TFDistilBertModel, DistilBertTokenizer, 'xlnet-base-cased')\n",
    "        model_class, tokenizer_class, pretrained_weights = model_desc\n",
    "        transformer = model_class.from_pretrained(pretrained_weights)\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        return transformer, tokenizer\n",
    "    \n",
    "    def compute_embeddings(self, input_ids):\n",
    "        num_batches = np.ceil(len(input_ids) / self.emb_batch_size)\n",
    "        batch_iter = tqdm(\n",
    "            range(0, len(input_ids)+1, self.emb_batch_size),\n",
    "            total=num_batches, desc='Embeddings')\n",
    "        embeddings = np.vstack([\n",
    "            self.transformer_eff(input_ids[n:n+self.emb_batch_size])\n",
    "            for n in batch_iter])\n",
    "        if not self.is_fitted:\n",
    "            self.scaler.fit(embeddings)\n",
    "            self.is_fitted = True\n",
    "        return self.scaler.transform(embeddings)\n",
    "    \n",
    "    def preprocess(self, data):\n",
    "        X, y = data\n",
    "        X_emb = self.preprocess_input(X)\n",
    "        y_onehot = self.one_hot_encoder.transform(y[:, np.newaxis]).todense()\n",
    "        return X_emb, y_onehot    \n",
    "    \n",
    "    def preprocess_input(self, x):\n",
    "        input_ids = [\n",
    "            self.tokenizer.encode(item, add_special_tokens=True) \n",
    "            for item in tqdm(x, desc='Tokenization')]\n",
    "        input_ids = pad_sequences(\n",
    "            input_ids, maxlen=self.max_sequence_length,\n",
    "            truncating='post', padding='post')\n",
    "        return self.compute_embeddings(input_ids)\n",
    "        \n",
    "    def build_model(self, input_shape):\n",
    "        inputs = layer = L.Input(shape=input_shape, dtype=float)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dropout(.2)(layer)\n",
    "        layer = L.Dense(self.class_count, activation='softmax')(inputs)\n",
    "        self.model = Model(inputs=inputs, outputs=layer)\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['acc'])        \n",
    "    \n",
    "    def train(self, epochs=40, batch_size=128):\n",
    "        print(\"Training\")\n",
    "        X_train, y_train = self.train_data\n",
    "        self.model.fit(X_train, y_train, epochs=epochs)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.train_data = self.preprocess((x, y))\n",
    "        self.idxmap = np.squeeze(\n",
    "            np.array([np.where(self.one_hot_encoder.categories_[0] == class_label)\n",
    "                      for class_label in self.classes]))        \n",
    "        self.build_model(self.train_data[0].shape[1])\n",
    "        self.train()        \n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        X_emb = self.preprocess_input(x)\n",
    "        probs = self.model.predict(X_emb)\n",
    "        return probs[:, self.idxmap]\n",
    "\n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfNaiveBayesModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classes, alpha=.42):\n",
    "        self.classes = np.array(classes)\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.naive = MultinomialNB(alpha=alpha)        \n",
    "    def fit(self, x, y):\n",
    "        vectors = self.tfidf.fit_transform(x)\n",
    "        self.naive.fit(vectors, y)\n",
    "        self.idxmap = np.squeeze(\n",
    "            np.array([np.where(self.naive.classes_ == class_label)\n",
    "                      for class_label in self.classes]))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        vectors = self.tfidf.transform(x)\n",
    "        probabilities = self.naive.predict_proba(vectors)\n",
    "        return probabilities[:, self.idxmap]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVotingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimators, classes):\n",
    "        self.estimators = estimators\n",
    "        self.classes = classes\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        for estimator in self.estimators:\n",
    "            estimator.fit(x, y)\n",
    "            \n",
    "    def predict_proba(self, x):\n",
    "        probs = np.array(\n",
    "            [estimator.predict_proba(x) for estimator in self.estimators])\n",
    "        probs = np.sum(probs, axis=0) / len(self.estimators)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes[idx]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    #TODO: DELETE THIS\n",
    "    classes = train_val_df.label.unique()\n",
    "    bert_model = TransformerModel(classes)\n",
    "    bert_model.fit(train_df.comment, train_df.label)\n",
    "    predictions = bert_model.predict(val_df.comment)\n",
    "    accuracy = np.sum(predictions == val_df.label) / len(val_df)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    sess = tf.InteractiveSession()\n",
    "    use = hub.Module('../input/universal-sentence-encoder/use/use')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    classes = train_val_df.label.unique()\n",
    "    use_model = USEModel(classes, use)\n",
    "    naive_model = TfidfNaiveBayesModel(classes)\n",
    "    classifiers = [use_model, naive_model]\n",
    "    voting = SimpleVotingClassifier(estimators=classifiers, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models and not submit_predictions:\n",
    "    voting.fit(train_df.comment, train_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models and not submit_predictions:\n",
    "    predictions = voting.predict(val_df.comment)\n",
    "    accuracy = np.sum(predictions == val_df.label) / len(val_df)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models and not submit_predictions:\n",
    "    predictions = voting.estimators[0].predict(val_df.comment)\n",
    "    accuracy = np.sum(predictions == val_df.label) / len(val_df)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if submit_predictions:\n",
    "    voting.fit(train_val_df.comment, train_val_df.label)\n",
    "    sanity_check_predictions = voting.predict(val_df.comment)\n",
    "    accuracy = np.sum(sanity_check_predictions == val_df.label) / len(val_df)\n",
    "    print(f\"Sanity check accuracy: {accuracy} (not a true validation accuracy)\")\n",
    "    \n",
    "    predictions = voting.predict(data_test)\n",
    "    with open(\"predictions.csv\", 'w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Id\", \"Category\"])\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            wr.writerow((i,prediction))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
