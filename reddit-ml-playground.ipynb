{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\n",
      "/kaggle/input/universal-sentence-encoder/use/use/tfhub_module.pb\n",
      "/kaggle/input/universal-sentence-encoder/use/use/saved_model.pb\n",
      "/kaggle/input/universal-sentence-encoder/use/use/variables/variables.index\n",
      "/kaggle/input/universal-sentence-encoder/use/use/variables/variables.data-00000-of-00001\n",
      "/kaggle/input/ift3395-ift6390-reddit-comments/data_test.pkl\n",
      "/kaggle/input/ift3395-ift6390-reddit-comments/sample_submission.csv\n",
      "/kaggle/input/ift3395-ift6390-reddit-comments/data_train.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import csv\n",
    "import gensim\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.backend import one_hot, clear_session\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "WORD2VEC_FILE = '/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() # Needed for TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "datasets = ['train', 'test']\n",
    "input_path = '/kaggle/input/ift3395-ift6390-reddit-comments/'\n",
    "data_train, data_test = [np.load(os.path.join(input_path, f'data_{dataset}.pkl'), allow_pickle=True) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Preparation\n",
    "\n",
    "Prior to attempting the use of machine learning models, the following operations will be performed on the raw data\n",
    "* Convert the data to a Pandas dataframe\n",
    "* Split the data into a test set and a validation set with 10% of the data being used as a validation set. This sample will be used to fit hyperparameters and/or to evaluate the relative performance of the different algorithms attempted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to true to allow for model training!\n",
    "train_models = False\n",
    "submit_predictions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_FRACTION = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(data):\n",
    "    if len(data) == 2:\n",
    "        comment, label = data\n",
    "        result = pd.DataFrame({'comment': comment, 'label': label})\n",
    "    else:\n",
    "        result = pd.DataFrame({'comment': data})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = (to_dataframe(data) \n",
    "                     for data in [data_train, data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size=VAL_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_WORD = re.compile(r'^[a-zA-Z]+')\n",
    "RE_URL = re.compile(r'\\w+://\\S+')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def filter_no_stopwords(token):\n",
    "    return token.lower() not in STOPWORDS\n",
    "\n",
    "def filter_words_only(token):\n",
    "    return RE_WORD.match(token)\n",
    "\n",
    "def transform_drop_urls(text):\n",
    "    return RE_URL.sub('', text)\n",
    "\n",
    "def transform_lowercase(value):\n",
    "    return value.lower()\n",
    "\n",
    "def transform_stem(value):\n",
    "    return STEMMER.stem(value)\n",
    "\n",
    "class Tokenizer:\n",
    "    DEFAULT_TEXT_TRANSFORMS = [transform_drop_urls]\n",
    "    DEFAULT_TOKEN_FILTERS = [filter_words_only, filter_no_stopwords]\n",
    "    DEFAULT_TOKEN_TRANSFORMS = [transform_lowercase]\n",
    "    def __init__(self):\n",
    "        self.text_transforms = self.DEFAULT_TEXT_TRANSFORMS\n",
    "        self.token_filters = self.DEFAULT_TOKEN_FILTERS\n",
    "        self.token_transforms = self.DEFAULT_TOKEN_TRANSFORMS\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        return (self.process_item(item) for item in show_progress(data, desc='Tokenization'))\n",
    "    \n",
    "    def transform(self, token):\n",
    "        for transform in self.token_transforms:\n",
    "            token = transform(token)\n",
    "        return token\n",
    "    \n",
    "    def process_item(self, text):\n",
    "        for text_transform in self.text_transforms:\n",
    "            text = text_transform(text)\n",
    "        tokens = word_tokenize(text)\n",
    "        return [\n",
    "            self.transform(token) for token in tokens\n",
    "            if all(\n",
    "                token_filter(token)\n",
    "                for token_filter in self.token_filters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfWord2VecTransformer:\n",
    "    def __init__(self, model=None):\n",
    "        if not model:\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_FILE, binary=True)\n",
    "        self.model = model \n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.default_shape = self.model.vectors[0].shape\n",
    "        self.default_vector = np.zeros(self.default_shape)        \n",
    "        \n",
    "    def get_tfidf_weight(self, word, weights):\n",
    "        idx = self.tfidf.vocabulary_.get(word)\n",
    "        return weights[0, idx] if idx else 0.        \n",
    "\n",
    "    def mean_word_vector_tfidf(self, words, weights):\n",
    "        words_in_vocab = [\n",
    "            word for word in words if word in self.model.vocab]\n",
    "        w2v_vectors = np.array([\n",
    "            self.model.get_vector(word) for word in words_in_vocab])\n",
    "        word_weights = np.array([\n",
    "            self.get_tfidf_weight(word, weights) for word in words_in_vocab])\n",
    "        vector = np.mean(w2v_vectors * word_weights[:, np.newaxis], axis=0) \n",
    "        if vector.shape != self.default_shape:\n",
    "            vector = self.default_vector        \n",
    "        return vector\n",
    "    \n",
    "    def to_strings(self, data):\n",
    "        return [\n",
    "            ' '.join(words)\n",
    "            for words in data\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        data_as_strings = self.to_strings(data)        \n",
    "        tfidf_weights = self.tfidf.fit_transform(data_as_strings)\n",
    "        return np.vstack([\n",
    "            self.mean_word_vector_tfidf(words, weights)\n",
    "            for words, weights in zip(data, tfidf_weights)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    word2vec = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_FILE, binary=True)\n",
    "else:\n",
    "    word2vec = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models: # This one takes a while to initialize\n",
    "    tfidf_transformer = TfidfWord2VecTransformer(word2vec)\n",
    "    fake_data = [\n",
    "        ['i', 'failed','theory', 'midterm'],\n",
    "        ['this', 'is', 'really', 'bad']\n",
    "    ]\n",
    "    values = tfidf_transformer(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_labels(labels):\n",
    "    return np.array(labels)[:, np.newaxis]\n",
    "\n",
    "def to_dense(*args):\n",
    "    return [item.todense() if hasattr(item, 'todense') else item for item in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNeuralNetworkModel:\n",
    "    def __init__(self, classes):\n",
    "        self.class_count = len(classes)\n",
    "        self.one_hot_encoder = OneHotEncoder()\n",
    "        self.one_hot_encoder.fit(vectorize_labels(classes))\n",
    "        self.classes = self.one_hot_encoder.categories_[0]\n",
    "        self.X_train, self.y_train, self.val = None, None, None\n",
    "        \n",
    "    def build_model(self, input_shape):\n",
    "        inputs = layer = L.Input(shape=input_shape, dtype=float)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dropout(.2)(layer)\n",
    "        layer = L.Dense(self.class_count, activation='softmax')(inputs)\n",
    "        self.model = Model(inputs=inputs, outputs=layer)\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['acc'])\n",
    "        \n",
    "    def preprocess_inputs(self, X):\n",
    "        transformer = TfidfWord2VecTransformer(word2vec)\n",
    "        tokenizer = Tokenizer()\n",
    "        tokens = [tokenizer.process_item(item) for item in X]\n",
    "        return transformer(tokens)\n",
    "    \n",
    "    def preprocess_labels(self, y):\n",
    "        return self.one_hot_encoder.transform(vectorize_labels(y))\n",
    "        \n",
    "    def preprocess(self, X, y):\n",
    "        return self.preprocess_inputs(X), self.preprocess_labels(y)\n",
    "    \n",
    "    def prepare(self, train, val):\n",
    "        X, y = train\n",
    "        self.X_train, self.y_train = to_dense(*self.preprocess(X, y))\n",
    "        self.build_model(self.X_train.shape[1])\n",
    "        X_val, Y_val = val\n",
    "        self.val = to_dense(*self.preprocess(X_val, Y_val))\n",
    "    \n",
    "    def train(self, epochs=1):\n",
    "        self.model.fit(self.X_train, self.y_train, validation_data=self.val, epochs=epochs)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        results = model.predict(X)\n",
    "        idx = np.argmax(results)\n",
    "        return self.classes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicNeuralNetworkModel(train_df.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    model.prepare((train_df.comment, train_df.label), (val_df.comment, val_df.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    model.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNeuralNetworkModelNoWord2Vec:\n",
    "    def __init__(self, classes, max_features=10000):\n",
    "        self.class_count = len(classes)\n",
    "        self.one_hot_encoder = OneHotEncoder()\n",
    "        self.one_hot_encoder.fit(vectorize_labels(classes))\n",
    "        self.classes = self.one_hot_encoder.categories_[0]\n",
    "        self.X_train, self.y_train, self.val = None, None, None\n",
    "        self.vectorizer = None\n",
    "        self.max_features = max_features\n",
    "        \n",
    "    def build_model(self, input_shape):\n",
    "        inputs = layer = L.Input(shape=input_shape, dtype=float)\n",
    "        layer = L.BatchNormalization()(inputs)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dropout(.2)(layer)\n",
    "        layer = L.Dense(self.class_count, activation='softmax')(inputs)\n",
    "        self.model = Model(inputs=inputs, outputs=layer)\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['acc'])\n",
    "        \n",
    "    def preprocess_inputs(self, X):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokens = [tokenizer.process_item(item) for item in X]\n",
    "        detokenized_comments = [' '.join(item) for item in tokens]\n",
    "        if self.vectorizer is None:\n",
    "            self.vectorizer = TfidfVectorizer(max_features=self.max_features)\n",
    "            result = self.vectorizer.fit_transform(detokenized_comments)\n",
    "        else:\n",
    "            result = self.vectorizer.transform(detokenized_comments)\n",
    "        return result\n",
    "    \n",
    "    def preprocess_labels(self, y):\n",
    "        return self.one_hot_encoder.transform(vectorize_labels(y))\n",
    "        \n",
    "    def preprocess(self, X, y):\n",
    "        return self.preprocess_inputs(X), self.preprocess_labels(y)\n",
    "    \n",
    "    def prepare(self, train, val):\n",
    "        X, y = train\n",
    "        self.X_train, self.y_train = to_dense(*self.preprocess(X, y))\n",
    "        self.build_model(self.X_train.shape[1])\n",
    "        X_val, Y_val = val\n",
    "        self.val = to_dense(*self.preprocess(X_val, Y_val))\n",
    "    \n",
    "    def train(self, epochs=1):\n",
    "        self.model.fit(self.X_train, self.y_train, validation_data=self.val, epochs=epochs)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        results = model.predict(X)\n",
    "        idx = np.argmax(results)\n",
    "        return self.classes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicNeuralNetworkModelNoWord2Vec(train_df.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    model.prepare((train_df.comment, train_df.label), (val_df.comment, val_df.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    model.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostModel:\n",
    "    def __init__(self, classes, min_df=2, max_features=None, dr_components=None):\n",
    "        self.vectorizer = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(classes)\n",
    "        self.classes = classes\n",
    "        self.max_features = max_features\n",
    "        self.svd = TruncatedSVD(n_components=dr_components) if dr_components else None\n",
    "        self.bst = None\n",
    "        self.min_df = min_df\n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        X, y = data\n",
    "        if not self.vectorizer:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=self.max_features, min_df=self.min_df,\n",
    "                stop_words='english', analyzer='word',\n",
    "                ngram_range=(1, 3), use_idf=1, smooth_idf=1,\n",
    "                sublinear_tf=1)\n",
    "            X_vec = self.vectorizer.fit_transform(tqdm(X))\n",
    "        else:\n",
    "            X_vec = self.vectorizer.transform(tqdm(X))\n",
    "        if self.svd:\n",
    "            X_vec = self.svd.fit_transform(X_vec)\n",
    "        y_le = self.label_encoder.transform(y)\n",
    "        return xgb.DMatrix(X_vec, label=y_le)\n",
    "        \n",
    "    def prepare(self, train, val):\n",
    "        self.train_data = self.preprocess(train)\n",
    "        self.val_data = self.preprocess(val)\n",
    "    \n",
    "    def train(self, rounds=1):\n",
    "        param = {\n",
    "            'max_depth': 8,\n",
    "            'gamma': 0.1,\n",
    "            'eta':0.3,\n",
    "            'objective':'multi:softmax',\n",
    "            'num_class': len(self.classes)}\n",
    "        self.bst = xgb.train(\n",
    "            param,\n",
    "            self.train_data,\n",
    "            num_boost_round=rounds,\n",
    "            xgb_model=self.bst,\n",
    "            evals=[(self.train_data, 'train'), \n",
    "                   (self.val_data, 'validation')])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBoostModel(train_df.label.unique())\n",
    "if train_models:\n",
    "    model.prepare((train_df.comment, train_df.label), (val_df.comment, val_df.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    model.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel:\n",
    "    def __init__(self, classes, vocabulary_size=10000, max_words_per_comment=250, embedding_dim=100):\n",
    "        self.sequencer = KerasTokenizer(num_words=vocabulary_size,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                                   lower=True)\n",
    "        self.max_words_per_comment = max_words_per_comment\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.classes = classes\n",
    "        self.one_hot_encoder = OneHotEncoder()\n",
    "        self.one_hot_encoder.fit(vectorize_labels(classes))        \n",
    "        self.embedding_dim = 100\n",
    "    \n",
    "    def process_item(self, item):\n",
    "        words = self.tokenizer.process_item(item)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def preprocess(self, data):\n",
    "        X, y = data\n",
    "        texts = [self.process_item(item) for item in X]\n",
    "        self.sequencer.fit_on_texts(texts)\n",
    "        X_seq = self.sequencer.texts_to_sequences(texts)\n",
    "        X_seq = pad_sequences(X_seq, self.max_words_per_comment)\n",
    "        y_onehot = self.one_hot_encoder.transform(vectorize_labels(y)).todense()\n",
    "        return X_seq, y_onehot\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        model = Sequential()\n",
    "        model.add(L.Embedding(self.vocabulary_size, self.embedding_dim, input_length=input_shape))\n",
    "        model.add(L.SpatialDropout1D(0.4))\n",
    "        model.add(L.LSTM(100, dropout=0.4, recurrent_dropout=0.4))\n",
    "        model.add(L.Dense(len(self.classes), activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def prepare(self, train, val):\n",
    "        self.train_data = self.preprocess(train)\n",
    "        self.val_data = self.preprocess(val)\n",
    "        X, _ = self.train_data\n",
    "        self.model = self.build_model(X.shape[1])\n",
    "        \n",
    "    def train(self, epochs=1):\n",
    "        X, y = self.train_data\n",
    "        self.model.fit(X, y, validation_data=self.val_data, epochs=epochs, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(train_df.label.unique())\n",
    "if train_models:\n",
    "    model.prepare((train_df.comment, train_df.label), (val_df.comment, val_df.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    model.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NbSvmModel:\n",
    "    def __init__(self, classes, C=1.0, dual=False,\n",
    "                 n_jobs=1, max_features=10000, min_df=2,\n",
    "                 dr_components=None):\n",
    "        self.classifier = NbSvmClassifier(C, dual, n_jobs)\n",
    "        self.classes = classes\n",
    "        self.vectorizer = None\n",
    "        self.max_features = max_features\n",
    "        self.min_df = min_df\n",
    "        self.svd = TruncatedSVD(n_components=dr_components) if dr_components else None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(classes)        \n",
    "    \n",
    "    def preprocess(self, data):\n",
    "        X, y = data\n",
    "        if not self.vectorizer:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=self.max_features, min_df=self.min_df,\n",
    "                stop_words='english', analyzer='word', use_idf=1, smooth_idf=1)\n",
    "            X_vec = self.vectorizer.fit_transform(tqdm(X))\n",
    "        else:\n",
    "            X_vec = self.vectorizer.transform(tqdm(X))\n",
    "        if self.svd:\n",
    "            X_vec = self.svd.fit_transform(X_vec)\n",
    "        y_le = self.label_encoder.transform(y)\n",
    "        return X_vec, y_le\n",
    "        \n",
    "    def prepare(self, train, val):\n",
    "        self.train_data = self.preprocess(train)\n",
    "        self.val_data = self.preprocess(val)        \n",
    "        \n",
    "    def train(self):\n",
    "        X, y = self.train_data\n",
    "        self.classifier.fit(X, y)\n",
    "        X_val, y_val = self.val_data\n",
    "        y_pred = self.classifier.predict(X_val)\n",
    "        correct = np.sum(y_pred == y_val)\n",
    "        acc = correct / len(y_val)\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "        \n",
    "    def predict(self, data):\n",
    "        y_pred = self.classifier.predict(data)\n",
    "        return [self.classes[idx] for idx in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NbSvmModel(train_df.label.unique())\n",
    "if train_models:\n",
    "    model.prepare((train_df.comment, train_df.label), (val_df.comment, val_df.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMModel:\n",
    "    def __init__(self, classes, max_features=10000, min_df=2,\n",
    "                 dr_components=None, scale=False, **kwargs):\n",
    "        self.classifier = SVC(**kwargs)\n",
    "        self.classes = classes\n",
    "        self.vectorizer = None\n",
    "        self.max_features = max_features\n",
    "        self.min_df = min_df\n",
    "        self.svd = TruncatedSVD(n_components=dr_components) if dr_components else None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(classes)        \n",
    "    \n",
    "    def preprocess(self, data):\n",
    "        X, y = data\n",
    "        if not self.vectorizer:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=self.max_features, min_df=self.min_df,\n",
    "                stop_words='english', analyzer='word', use_idf=1, smooth_idf=1)\n",
    "            X_vec = self.vectorizer.fit_transform(tqdm(X))\n",
    "            if self.scale:\n",
    "                X_vec = self.scaler.fit_transform(X_vec.todense())\n",
    "        else:\n",
    "            X_vec = self.vectorizer.transform(tqdm(X))\n",
    "            if self.scale:\n",
    "                X_vec = self.scaler.transform(X_vec.todense())\n",
    "        if self.svd:\n",
    "            X_vec = self.svd.fit_transform(X_vec)\n",
    "        y_le = self.label_encoder.transform(y)\n",
    "        return X_vec, y_le\n",
    "        \n",
    "    def prepare(self, train, val):\n",
    "        self.train_data = self.preprocess(train)\n",
    "        self.val_data = self.preprocess(val)        \n",
    "        \n",
    "    def train(self):\n",
    "        X, y = self.train_data\n",
    "        self.classifier.fit(X, y)\n",
    "        X_val, y_val = self.val_data\n",
    "        y_pred = self.classifier.predict(X_val)\n",
    "        correct = np.sum(y_pred == y_val)\n",
    "        acc = correct / len(y_val)\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "        \n",
    "    def predict(self, data):\n",
    "        y_pred = self.classifier.predict(data)\n",
    "        return [self.classes[idx] for idx in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVMModel(train_df.label.unique(), max_iter=100, verbose=True, cache_size=7000, kernel='sigmoid')\n",
    "if train_models:\n",
    "    model.prepare((train_df.comment, train_df.label), (val_df.comment, val_df.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = hub.Module('../input/universal-sentence-encoder/use/use')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USEModel:\n",
    "    def __init__(self, classes, use=None):\n",
    "        if not use:\n",
    "            use = hub.Module('../input/universal-sentence-encoder/use/use')\n",
    "        self.classes = classes\n",
    "        self.class_count = len(classes)\n",
    "        self.use = use\n",
    "        self.one_hot_encoder = OneHotEncoder()\n",
    "        self.one_hot_encoder.fit(vectorize_labels(classes))\n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        X, y = data\n",
    "        X_emb = sess.run(self.use(X))\n",
    "        y_onehot = self.one_hot_encoder.transform(y[:, np.newaxis]).todense()\n",
    "        return X_emb, y_onehot\n",
    "    \n",
    "    def prepare(self, train, val):\n",
    "        self.train_data = self.preprocess(train)\n",
    "        self.val_data = self.preprocess(val)\n",
    "        self.build_model(self.train_data[0].shape[1])\n",
    "        \n",
    "    def build_model(self, input_shape):\n",
    "        inputs = layer = L.Input(shape=input_shape, dtype=float)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dense(2048, activation='relu')(layer)\n",
    "        layer = L.Dropout(.2)(layer)\n",
    "        layer = L.Dense(self.class_count, activation='softmax')(inputs)\n",
    "        self.model = Model(inputs=inputs, outputs=layer)\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['acc'])        \n",
    "\n",
    "    def train(self, epochs=1):\n",
    "        print(\"Training\")\n",
    "        X_train, y_train = self.train_data\n",
    "        self.model.fit(X_train, y_train, validation_data=self.val_data, epochs=epochs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_emb = sess.run(self.use(X))        \n",
    "        results = self.model.predict(X_emb)\n",
    "        return np.squeeze(self.one_hot_encoder.inverse_transform(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = USEModel(train_val_df.label.unique(), use)\n",
    "if train_models:\n",
    "    model.prepare((train_df.comment, train_df.label), (val_df.comment, val_df.label))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    model.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train on 70000 samples, validate on 7000 samples\n",
      "Epoch 1/40\n",
      "70000/70000 [==============================] - 6s 83us/sample - loss: 1.7706 - acc: 0.5767 - val_loss: 1.3973 - val_acc: 0.6047\n",
      "Epoch 2/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.3410 - acc: 0.6045 - val_loss: 1.3241 - val_acc: 0.6056\n",
      "Epoch 3/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.3023 - acc: 0.6087 - val_loss: 1.3050 - val_acc: 0.6103\n",
      "Epoch 4/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2892 - acc: 0.6116 - val_loss: 1.2960 - val_acc: 0.6116\n",
      "Epoch 5/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2818 - acc: 0.6127 - val_loss: 1.2910 - val_acc: 0.6131\n",
      "Epoch 6/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2770 - acc: 0.6147 - val_loss: 1.2871 - val_acc: 0.6140\n",
      "Epoch 7/40\n",
      "70000/70000 [==============================] - 5s 73us/sample - loss: 1.2734 - acc: 0.6152 - val_loss: 1.2838 - val_acc: 0.6159\n",
      "Epoch 8/40\n",
      "70000/70000 [==============================] - 5s 74us/sample - loss: 1.2707 - acc: 0.6158 - val_loss: 1.2811 - val_acc: 0.6156\n",
      "Epoch 9/40\n",
      "70000/70000 [==============================] - 5s 74us/sample - loss: 1.2682 - acc: 0.6167 - val_loss: 1.2799 - val_acc: 0.6160\n",
      "Epoch 10/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2660 - acc: 0.6171 - val_loss: 1.2781 - val_acc: 0.6169\n",
      "Epoch 11/40\n",
      "70000/70000 [==============================] - 5s 73us/sample - loss: 1.2642 - acc: 0.6179 - val_loss: 1.2756 - val_acc: 0.6166\n",
      "Epoch 12/40\n",
      "70000/70000 [==============================] - 5s 74us/sample - loss: 1.2624 - acc: 0.6186 - val_loss: 1.2740 - val_acc: 0.6174\n",
      "Epoch 13/40\n",
      "70000/70000 [==============================] - 5s 73us/sample - loss: 1.2610 - acc: 0.6183 - val_loss: 1.2725 - val_acc: 0.6184\n",
      "Epoch 14/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2595 - acc: 0.6188 - val_loss: 1.2711 - val_acc: 0.6171\n",
      "Epoch 15/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.2581 - acc: 0.6194 - val_loss: 1.2710 - val_acc: 0.6181\n",
      "Epoch 16/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2568 - acc: 0.6197 - val_loss: 1.2695 - val_acc: 0.6193\n",
      "Epoch 17/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2556 - acc: 0.6202 - val_loss: 1.2682 - val_acc: 0.6193\n",
      "Epoch 18/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2546 - acc: 0.6201 - val_loss: 1.2681 - val_acc: 0.6220\n",
      "Epoch 19/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2535 - acc: 0.6208 - val_loss: 1.2657 - val_acc: 0.6207\n",
      "Epoch 20/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.2524 - acc: 0.6213 - val_loss: 1.2646 - val_acc: 0.6217\n",
      "Epoch 21/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2514 - acc: 0.6214 - val_loss: 1.2636 - val_acc: 0.6194\n",
      "Epoch 22/40\n",
      "70000/70000 [==============================] - 5s 74us/sample - loss: 1.2505 - acc: 0.6213 - val_loss: 1.2628 - val_acc: 0.6210\n",
      "Epoch 23/40\n",
      "70000/70000 [==============================] - 5s 74us/sample - loss: 1.2495 - acc: 0.6225 - val_loss: 1.2618 - val_acc: 0.6197\n",
      "Epoch 24/40\n",
      "70000/70000 [==============================] - 5s 74us/sample - loss: 1.2487 - acc: 0.6224 - val_loss: 1.2613 - val_acc: 0.6204\n",
      "Epoch 25/40\n",
      "70000/70000 [==============================] - 5s 73us/sample - loss: 1.2478 - acc: 0.6225 - val_loss: 1.2597 - val_acc: 0.6214\n",
      "Epoch 26/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.2470 - acc: 0.6224 - val_loss: 1.2597 - val_acc: 0.6204\n",
      "Epoch 27/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2462 - acc: 0.6230 - val_loss: 1.2586 - val_acc: 0.6224\n",
      "Epoch 28/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.2454 - acc: 0.6228 - val_loss: 1.2579 - val_acc: 0.6220\n",
      "Epoch 29/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.2447 - acc: 0.6230 - val_loss: 1.2576 - val_acc: 0.6234\n",
      "Epoch 30/40\n",
      "70000/70000 [==============================] - 5s 73us/sample - loss: 1.2439 - acc: 0.6231 - val_loss: 1.2569 - val_acc: 0.6206\n",
      "Epoch 31/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.2432 - acc: 0.6236 - val_loss: 1.2554 - val_acc: 0.6213\n",
      "Epoch 32/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.2424 - acc: 0.6236 - val_loss: 1.2543 - val_acc: 0.6237\n",
      "Epoch 33/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.2419 - acc: 0.6243 - val_loss: 1.2540 - val_acc: 0.6233\n",
      "Epoch 34/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.2412 - acc: 0.6246 - val_loss: 1.2534 - val_acc: 0.6224\n",
      "Epoch 35/40\n",
      "70000/70000 [==============================] - 5s 71us/sample - loss: 1.2405 - acc: 0.6239 - val_loss: 1.2533 - val_acc: 0.6230\n",
      "Epoch 36/40\n",
      "70000/70000 [==============================] - 5s 74us/sample - loss: 1.2400 - acc: 0.6248 - val_loss: 1.2523 - val_acc: 0.6226\n",
      "Epoch 37/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2392 - acc: 0.6242 - val_loss: 1.2518 - val_acc: 0.6237\n",
      "Epoch 38/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2386 - acc: 0.6248 - val_loss: 1.2511 - val_acc: 0.6226\n",
      "Epoch 39/40\n",
      "70000/70000 [==============================] - 5s 73us/sample - loss: 1.2381 - acc: 0.6248 - val_loss: 1.2496 - val_acc: 0.6237\n",
      "Epoch 40/40\n",
      "70000/70000 [==============================] - 5s 72us/sample - loss: 1.2375 - acc: 0.6250 - val_loss: 1.2502 - val_acc: 0.6230\n"
     ]
    }
   ],
   "source": [
    "if submit_predictions:\n",
    "    model.prepare((train_val_df.comment, train_val_df.label), (val_df.comment, val_df.label))\n",
    "    model.train(40)\n",
    "    predictions = model.predict(data_test)\n",
    "    with open(\"predictions.csv\", 'w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Id\", \"Category\"])\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            wr.writerow((i,prediction))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
