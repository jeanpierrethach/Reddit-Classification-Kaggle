{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Comments Classification Competition\n",
    "\n",
    "## Authors\n",
    "* Artem Ploujnikov\n",
    "* Jean-Pierre Thach\n",
    "\n",
    "## Overview\n",
    "This notebook contains the final solution consisting of a weighted ensemble model of the following machine learning algorithms:\n",
    "\n",
    "* A Multilevel Perceptron / Fully Connected Neural Network trained using features extracted via the pre-trained Google Universal Sentence Embedding model\n",
    "* Multinomial Naive Bayes (trained on TFIDF-weighted Bag of Words features combined with additional engineered features)\n",
    "* A Linear Support Vector Machine\n",
    "\n",
    "## Running Instructions\n",
    "This notebook was extracted from a Kaggle kernel. To reproduce the results, follow the steps described below:\n",
    "\n",
    "- Navigate to the competition in Kaggle.\n",
    "- Create a new Python Notebook kernel in the competition. This will automatically attach the Reddit dataset.\n",
    "- Attach the Swear Words dataset to the kernel: https://www.kaggle.com/highflyingbird/swear-words\n",
    "- Turn on the Internet. This is required to download the pretrained Universal Sentence Encoder model and install the `textstat` package. The notebook does not rely on any external data.\n",
    "- Turn on the GPU. Without GPU acceleration, training the Multilevel Perceptron model and extracting feature vectors from the\n",
    "- Click on Commit to run the notebook top-to-bottom or run or run it interactively and download `submissions.csv`\n",
    "\n",
    "Running this notebook outside of Kaggle is possible but requires adjustments: file paths would need to be changed, and dependencies would need to be installed. Running it on configurations that are significantly different from that of the Kaggle kernel (e.g. older versions of Python) might not be possible.\n",
    "\n",
    "## Not Included\n",
    "This notebook only includes the models that were used to produce the final submission. The notebook does not include:\n",
    "\n",
    "* The Explorartory Data Anlaysis\n",
    "* Any models and techniques that were not selected for the final submission (but may appear on the report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "Running this notebook requires the following libraries:\n",
    "* `numpy`\n",
    "* `pandas`\n",
    "* `scikit-learn`\n",
    "* `tensorflow` (version 2, run in compatibility mode with version `)\n",
    "* `nltk`\n",
    "* `textstat`\n",
    "\n",
    "All libraries except `textstat` are preinstalled on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "colab_type": "code",
    "id": "xFpWFXM_kSy0",
    "outputId": "d22b8af8-96b4-45b9-bb6b-74610fe83656"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "!pip install textstat\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.backend import one_hot, clear_session\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import textstat\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `submit_predictions` flag should be set to `true` when generating final submissions. It can be set to `false` during experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgQkgQnIkSy_"
   },
   "outputs": [],
   "source": [
    "submit_predictions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Kaggle is preinstalled with TensorFlow 2 by default. At the time of writing, many TensorFlow Hub models, including the Universal Sentence Encoder, were not Tensorflow 2-compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "zoM2bS4ekSzG",
    "outputId": "60f9ec0b-ff95-4f9b-d44d-536416578366"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() # Needed for TensorFlow Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "V8AG0QnjkSzM",
    "outputId": "af62e041-8400-4484-fc90-50f72171974a"
   },
   "outputs": [],
   "source": [
    "datasets = ['train', 'test']\n",
    "data_train = np.load(\"/kaggle/input/ift3395-ift6390-reddit-comments/data_train.pkl\", allow_pickle=True)\n",
    "data_test = np.load(\"/kaggle/input/ift3395-ift6390-reddit-comments/data_test.pkl\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHxXKtw0kSzS"
   },
   "outputs": [],
   "source": [
    "def to_dataframe(data):\n",
    "    if len(data) == 2:\n",
    "        comment, label = data\n",
    "        result = pd.DataFrame({'comment': comment, 'label': label})\n",
    "    else:\n",
    "        result = pd.DataFrame({'comment': data})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXrHAyZfylP8"
   },
   "outputs": [],
   "source": [
    "def read_word_list(file_name):\n",
    "\t\"\"\"\n",
    "\tThis function reads a list of words and return it as a set\n",
    "\n",
    "\tParameter:\n",
    "\t\t\tfile_name\n",
    "\tReturns a set of the words\n",
    "\t\"\"\"\n",
    "\twith open(file_name) as word_list_file:\n",
    "\t\treturn set(word.strip() for word in word_list_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Calling `to_dataframe` produces a Pandas DataFrame with the ofllowing columns:\n",
    "* `comment`: The raw comment text\n",
    "* `label`: The string label\n",
    "\n",
    "### Basic Features\n",
    "\n",
    "Running `enhance_tokenization` adds the following features to the dataframe:\n",
    "* `words`: the comment converted to a list of words using `word_tokenize` from `NLTK`\n",
    "* `sentences`: the comment converted to a list of sentences using `sent_tokenize` from `NLTK`\n",
    "* `length`: the length, in characters\n",
    "* `word_count`: the number of words in the comment\n",
    "* `sentence_count`: the number of sentences in teh comment\n",
    "\n",
    "### Swear Word Analysis\n",
    "Running `enhance_bad_words` adds the following features:\n",
    "* `has_bad_words`: a Boolean value indicating whether the comment contains any profanity\n",
    "* `bad_word_count`: the number of swear words in the comment\n",
    "\n",
    "### Readability Analysis\n",
    "Running `enhance_readability` adds the following features:\n",
    "* `flesch_reading_ease`: the Flesch reading ease metric\n",
    "* `difficult_words`: the number of difficult words\n",
    "\n",
    "The Exploratory Data Analysis included additional metrics. Only metrics selected for machine learning are being used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqqbQN9Lx3lB"
   },
   "outputs": [],
   "source": [
    "def enhance_tokenization(df):\n",
    "    \"\"\"\n",
    "    This function enhances the dataframe with the length of comments,\n",
    "    the words tokenized, the sentences tokenized, tokens processed by\n",
    "    our Tokenizer, the word count and the sentence count.\n",
    "\n",
    "    Parameter:\n",
    "        df:  dataframe\n",
    "\n",
    "    Returns the enhanced dataframe\n",
    "    \"\"\"\n",
    "    dataframe = df\n",
    "    dataframe['length'] = df.comment.str.len()\n",
    "    tqdm.pandas('Tokenizing Words')\n",
    "    dataframe['words'] = df.comment.progress_apply(word_tokenize)\n",
    "    tqdm.pandas('Tokenizing Sentences')\n",
    "    dataframe['sentences'] = df.comment.progress_apply(sent_tokenize)\n",
    "    dataframe['word_count'] = df.words.apply(len)\n",
    "    dataframe['sentence_count'] = df.sentences.apply(len)\n",
    "    return dataframe\n",
    "\n",
    "BAD_WORD_LIST = '/kaggle/input/swear-words/badwords.txt'\n",
    "def enhance_bad_words(df):\n",
    "    \"\"\"\n",
    "    This function enhances the dataframe with the count of\n",
    "    bad words and a boolean indicating the appearance of bad word.\n",
    "\n",
    "    Parameter:\n",
    "        df: dataframe\n",
    "\n",
    "    Returns the enhanced dataframe\n",
    "    \"\"\"\n",
    "    dataframe = df\n",
    "    bad_words = read_word_list(BAD_WORD_LIST)\n",
    "    dataframe['bad_word_count'] = df.words.apply(lambda words: len(set(word.lower() for word in words) & bad_words))\n",
    "    dataframe['has_bad_words'] = df.bad_word_count > 0\n",
    "    return dataframe\n",
    "\n",
    "readability_stats = [\n",
    "    ('flesch_reading_ease', 'Flesch Reading Ease'),\n",
    "    ('difficult_words', 'Difficult Words')]\n",
    "\n",
    "def enhance_readability(df):\n",
    "    \"\"\"\n",
    "    This function enhances the dataframe with the flesch reading ease,\n",
    "    the smog index, the flesch kincaid grade, the difficulty of words\n",
    "    from textstat.\n",
    "\n",
    "    Parameter:\n",
    "        df: dataframe\n",
    "\n",
    "    Returns the enhanced dataframe\n",
    "    \"\"\"\n",
    "    dataframe = df\n",
    "    for item in readability_stats:\n",
    "        key, label = item\n",
    "        #tqdm.pandas(desc=label)\n",
    "        stat = getattr(textstat, key)\n",
    "        dataframe[key] = df.comment.apply(stat)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Validation Split\n",
    "\n",
    "The following split procedure will yield the following dataframes:\n",
    "\n",
    "* `train_df`: the training set (90% of the training data provided)\n",
    "* `val_df`: the validation set (10% of the training data provided)\n",
    "* `test_df`: the true test set (used for submissions only)\n",
    "* `train_val_df`: the entire training data set (used for final submissions only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_JfJnXEHkSzZ"
   },
   "outputs": [],
   "source": [
    "VAL_FRACTION = 0.1\n",
    "train_val_df, test_df = (to_dataframe(data) \n",
    "                     for data in [data_train, data_test])\n",
    "\n",
    "train_val_df = enhance_tokenization(train_val_df)\n",
    "train_val_df = enhance_bad_words(train_val_df)\n",
    "train_val_df = enhance_readability(train_val_df)\n",
    "\n",
    "test_df = enhance_tokenization(test_df)\n",
    "test_df = enhance_bad_words(test_df)\n",
    "test_df = enhance_readability(test_df)\n",
    "\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=VAL_FRACTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "* `NumberSelector`: A transformer that selects a numeric feature out of a dataframe (used in sklearn pipelining)\n",
    "* `Textselector`: A transformer that selects a textual feature out of a dataframe (used in sklearn pipelining)\n",
    "* `vectorize_labels`: Converts a list of labels (or any other collection) to a Nx1 `numpy` array\n",
    "* `to_dense`: Converts sparse matrices to `numpy` arrays if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fLCYCWS1KCh"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2AhEX5dkSzd"
   },
   "outputs": [],
   "source": [
    "def vectorize_labels(labels):\n",
    "    return np.array(labels)[:, np.newaxis]\n",
    "\n",
    "def to_dense(*args):\n",
    "    return [item.todense() if hasattr(item, 'todense') else item for item in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Universal Sentence Encoder Model\n",
    "\n",
    "This model extracts feature vectors from Google's Universal Sentence Encoder pre-trained model and trains a simple  fully-connected neural network with 2 hidden layers:\n",
    "\n",
    "* Inputs: Same as Universal Sentence Encoder outputs\n",
    "* Layers:\n",
    "  * Fully Connected (Dense), 1024 units, ReLu activation\n",
    "  * Fully Connected (Dense), 1024 units, ReLu activation\n",
    "  * Fully Connected (Dense), 20 units, Softmax activation\n",
    "* Training\n",
    "  * Optimizer: Adam\n",
    "  * Loss: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AdDe-mwakSzh"
   },
   "outputs": [],
   "source": [
    "class USEModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classes, use=None):\n",
    "        if not use:\n",
    "            use = hub.Module('https://tfhub.dev/google/universal-sentence-encoder/2')\n",
    "        self.classes = classes\n",
    "        self.class_count = len(classes)\n",
    "        self.use = use\n",
    "        self.one_hot_encoder = OneHotEncoder()\n",
    "        self.one_hot_encoder.fit(vectorize_labels(classes))\n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        X, y = data\n",
    "        X_emb = sess.run(self.use(X.tolist()))\n",
    "        y_onehot = self.one_hot_encoder.transform(y[:, np.newaxis]).todense()\n",
    "        return X_emb, y_onehot\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.train_data = self.preprocess((x.comment, y))\n",
    "        self.idxmap = np.squeeze(\n",
    "            np.array([np.where(self.one_hot_encoder.categories_[0] == class_label)\n",
    "                      for class_label in self.classes]))        \n",
    "        self.build_model(self.train_data[0].shape[1])\n",
    "        self.train()\n",
    "        \n",
    "    def build_model(self, input_shape):\n",
    "        inputs = layer = L.Input(shape=input_shape, dtype=float)\n",
    "        layer = L.Dense(1024, activation='relu')(layer)\n",
    "        layer = L.Dense(1024, activation='relu')(layer)\n",
    "        layer = L.Dropout(.1)(layer)\n",
    "        layer = L.Dense(self.class_count, activation='softmax')(inputs)\n",
    "        self.model = Model(inputs=inputs, outputs=layer)\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['acc'])        \n",
    "\n",
    "    def train(self, epochs=40):\n",
    "        print(\"Training\")\n",
    "        X_train, y_train = self.train_data\n",
    "        history = self.model.fit(X_train, y_train, validation_data=self.preprocess((val_df.comment, val_df.label)), epochs=epochs)\n",
    "        print(history.history.keys())\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_emb = sess.run(self.use(X.comment))        \n",
    "        probs = self.model.predict(X_emb)\n",
    "        return probs[:, self.idxmap]\n",
    "\n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes[idx]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Class for Simple Linear Models\n",
    "\n",
    "This is not a complete model, but rather, a reusable class that provides feature pipelining using TFIDF-weighted Bag of Words features pipelined together with the hand-engineered length, swear word and readability features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBaseModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classes):\n",
    "        self.classes = np.array(classes)\n",
    "        self.tfidf = TfidfVectorizer(stop_words=STOPWORDS, strip_accents='unicode')\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "\n",
    "        p_comment = Pipeline([\n",
    "                ('selector', TextSelector(key='comment')),\n",
    "                ('tfidf', TfidfVectorizer(stop_words=STOPWORDS, strip_accents='unicode'))])\n",
    "        p_length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='length')),\n",
    "                ('standard', MinMaxScaler())])\n",
    "        p_word_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='word_count')),\n",
    "                ('standard', MinMaxScaler())])\n",
    "        p_sentence_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='sentence_count')),\n",
    "                ('standard', MinMaxScaler())])\n",
    "        p_flesch_reading_ease =  Pipeline([\n",
    "                ('selector', NumberSelector(key='flesch_reading_ease')),\n",
    "                ('standard', MinMaxScaler())])\n",
    "        p_difficult_words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='difficult_words')),\n",
    "                ('standard', MinMaxScaler())])\n",
    "        p_bad_word_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='bad_word_count')),\n",
    "                ('standard', MinMaxScaler())])\n",
    "        \n",
    "        print(\"Transforming comments\")\n",
    "        p_comment.fit_transform(x)\n",
    "        print(\"Transforming length\")\n",
    "        p_length.fit_transform(x)\n",
    "        print(\"Transforming word counts\")\n",
    "        p_word_count.fit_transform(x)\n",
    "        print(\"Transforming sentence counts\")\n",
    "        p_sentence_count.fit_transform(x)\n",
    "        print(\"Transforming reading ease\")\n",
    "        p_flesch_reading_ease.fit_transform(x)\n",
    "        print(\"Transforming difficult words\")\n",
    "        p_difficult_words.fit_transform(x)\n",
    "        print(\"Transforming bad words\")\n",
    "        p_bad_word_count.fit_transform(x)\n",
    "\n",
    "        all_features = FeatureUnion([('comment', p_comment),\n",
    "                                    ('length', p_length),\n",
    "                                    ('word_count', p_word_count),\n",
    "                                    ('sentence_count', p_sentence_count),\n",
    "                                    ('flesch_reading_ease', p_flesch_reading_ease),\n",
    "                                    ('difficult_words', p_difficult_words),\n",
    "                                    ('bad_word_count', p_bad_word_count),\n",
    "                                    ])\n",
    "        self.p_features = Pipeline([('all_features', all_features)])\n",
    "        self.p_features.fit_transform(x)\n",
    "       \n",
    "        self.classifier = Pipeline([\n",
    "            ('all_features', self.p_features),\n",
    "            ('classifier', self.get_classifier())\n",
    "        ])\n",
    "        self.classifier.fit(x, y)\n",
    "        self.idxmap = np.squeeze(\n",
    "            np.array([np.where(self.classifier.classes_ == class_label)\n",
    "                      for class_label in self.classes]))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        probabilities = self.classifier.predict_proba(x)\n",
    "        return probabilities[:, self.idxmap]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes[idx]\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Naive Bayes Model\n",
    "\n",
    "A simple Multinomial Naive Bayes baseline model using the features available in the `SimpleBaseModel` class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "124ZlwvVkSzo"
   },
   "outputs": [],
   "source": [
    "class TfidfNaiveBayesModel(SimpleBaseModel):\n",
    "    def __init__(self, classes, alpha=0.2783577600782345):\n",
    "        super().__init__(classes)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def get_classifier(self):\n",
    "        return MultinomialNB(alpha=self.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "\n",
    "A simple linear model based on the `sklearn` `SGDClassifier` using the features available in the `SimpleBaseModel` class above. The underlying classifier provides implementation for logistic regression and support vector machines. A support vector machine is used in this implementation.\n",
    "\n",
    "Given below are the hyperparameter values that were used for this model:\n",
    "* `alpha=1e-3`\n",
    "* `random_state=42` \n",
    "* `max_iter=400`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(SimpleBaseModel):\n",
    "    def __init__(self, classes, **hparams):\n",
    "        super().__init__(classes)        \n",
    "        self.hparams = hparams\n",
    "    \n",
    "    def get_classifier(self):\n",
    "        return SGDClassifier(**self.hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Voting Ensembling Meta-Classifier\n",
    "\n",
    "This meta-classifier combines the results of multiple classifiers via soft voting, optionally weighting the classifiers in proportion to their performance of the training set.\n",
    "\n",
    "$$c_{pred} = arg \\max_c \\frac{1}{n_c} \\sum_{i=1}^{n_c} \\alpha_i P_i(y=c) $$\n",
    "\n",
    "When `weighted==True`:\n",
    "$$\n",
    "\\alpha_i = \\frac{1}{2}\\ln \\frac{1 âˆ’ \\epsilon_i} {\\epsilon_i}\n",
    "$$\n",
    "\n",
    "When `weighted==False`\n",
    "\n",
    "$$\n",
    "\\forall i, \\alpha_i = 1 \n",
    "$$\n",
    "\n",
    "Limitations:\n",
    "* `predict_proba` is not normalized when classifiers are weighted.\n",
    "* The weighting procedure is not suitable for classifiers achieving an error rate exceeding 50%.\n",
    "  * The formula was adopted form AdaBoost. The limitation can be addressed by revising it for multiclass classification; however, this limitation does not affect this particular classification problem given the observed error rates.\n",
    "* Adding that overfit significantly will hurt performance when weighting is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MCsI91BkSzt"
   },
   "outputs": [],
   "source": [
    "class SimpleVotingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimators, classes, weighted=True, sample=.25):\n",
    "        self.estimators = estimators\n",
    "        self.classes = classes\n",
    "        self.weighted = weighted\n",
    "        self.sample = sample\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        for estimator in self.estimators:\n",
    "            estimator.fit(x, y)\n",
    "        self.alphas = self.get_alpha_vector(x, y)[:, np.newaxis, np.newaxis]            \n",
    "\n",
    "    def get_sample(self, x, y):\n",
    "        data_size = len(x)\n",
    "        sample_size = math.ceil(data_size * self.sample)\n",
    "        idx = np.random.choice(np.arange(data_size), sample_size, replace=False)\n",
    "        return self.subset(x, idx), self.subset(y, idx)\n",
    "    \n",
    "    def subset(self, data, idx):\n",
    "        return data.iloc[idx] if hasattr(data, 'iloc') else data[idx]\n",
    "                    \n",
    "    def get_alpha(self, estimator, x, y):\n",
    "        sample_x, sample_y = self.get_sample(x, y)\n",
    "        predictions = estimator.predict(sample_x)\n",
    "        error_rate = np.sum(sample_y != predictions) / len(sample_y)\n",
    "        return .5 * np.log((1 - error_rate) / error_rate)\n",
    "    \n",
    "    def get_alpha_vector(self, x, y):\n",
    "        if self.weighted:\n",
    "            alpha = np.array([self.get_alpha(estimator, x, y)\n",
    "                         for estimator in self.estimators])\n",
    "        else:\n",
    "            alpha = np.ones(len(self.estimators))\n",
    "        return alpha\n",
    "            \n",
    "    def predict_proba(self, x):\n",
    "        probs = np.array(\n",
    "            [estimator.predict_proba(x) for estimator in self.estimators])\n",
    "        probs = np.sum(self.alphas * probs, axis=0) / len(self.estimators)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes[idx]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Sentence Encoder Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnVY8WpTkSzx"
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "use = hub.Module('https://tfhub.dev/google/universal-sentence-encoder/2')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling Mode Set-Up\n",
    "* Set up a weighted classifier with the following models:\n",
    "  * MLP with Universal Sentence Encoder features\n",
    "  * Naive Bayes with TFIDF + readability + swear words + length\n",
    "  * Linear SVM with TFIDF + readability + swear words + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2q0_UH8bkSz4"
   },
   "outputs": [],
   "source": [
    "classes = train_val_df.label.unique()\n",
    "use_model = USEModel(classes, use)\n",
    "naive_model = TfidfNaiveBayesModel(classes)\n",
    "linear_model = LinearModel(classes, loss='modified_huber', penalty='l2',alpha=1e-3, random_state=42, max_iter=400, tol=None, verbose=1)\n",
    "classifiers = [use_model, linear_model, naive_model]\n",
    "voting = SimpleVotingClassifier(estimators=classifiers, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation Mode\n",
    "\n",
    "Train the model on the training set, display the cross-validation error. This was used while developing the models and evaluating different candidates.\n",
    "\n",
    "When `submit_predictions` is set to `False`, the test data set is not used at all, and no submission is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BEADjQ2kSz9"
   },
   "outputs": [],
   "source": [
    "if not submit_predictions:\n",
    "    voting.fit(train_df, train_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLXc9tOnkS0G"
   },
   "outputs": [],
   "source": [
    "if not submit_predictions:\n",
    "    predictions = voting.predict(val_df)\n",
    "    accuracy = np.sum(predictions == val_df.label) / len(val_df)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Mode\n",
    "* Train the model on the entire test set\n",
    "* Compute a \"validation\" set accuracy for the sanity check. This is not a true validation error because the validation set has been used in training; rather, it is a training error computed on a sample. It can be useful only to detect potential problems with the implementation\n",
    "* Output `predictions.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAK1CI4VkS0N"
   },
   "outputs": [],
   "source": [
    "if submit_predictions:\n",
    "    voting.fit(train_val_df, train_val_df.label)\n",
    "    sanity_check_predictions = voting.predict(val_df)\n",
    "    accuracy = np.sum(sanity_check_predictions == val_df.label) / len(val_df)\n",
    "    print(f\"Sanity check accuracy: {accuracy} (not a true validation accuracy)\")\n",
    "    print(train_val_df.shape)\n",
    "    print(test_df.shape)\n",
    "    predictions = voting.predict(test_df)\n",
    "    with open(\"predictions.csv\", 'w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Id\", \"Category\"])\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            wr.writerow((i,prediction))  \n",
    "            print('{0},{1}'.format(i,prediction)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Download\n",
    "The `FileLink` widget will make it possible to download the submission file if the notebook is being run interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='predictions.csv' target='_blank'>predictions.csv</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/predictions.csv"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ensembling-experimentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
